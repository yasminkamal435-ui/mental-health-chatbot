# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KwNvsGvzTcJDQjcu9lWIq8kzQBxDa6Qc
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import io
import joblib
from datetime import datetime
from io import BytesIO
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

try:
    from xgboost import XGBClassifier
    xgb_available = True
except Exception:
    xgb_available = False

try:
    import lightgbm as lgb
    lgb_available = True
except Exception:
    lgb_available = False

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    tf_available = True
except Exception:
    tf_available = False

try:
    from textblob import TextBlob
    textblob_available = True
except Exception:
    textblob_available = False

st.set_page_config(page_title=" AI Mental Health Intelligence (Advanced)", layout="wide", page_icon="🧩")
st.title(" AI-Powered Mental Health & Lifestyle Intelligence (Advanced)")

st.markdown("""
**Overview:** نظام متكامل لتحليل بيانات الصحة النفسية والسلوكية (EDA + ML + DL + NLP + Clustering + Prediction + Reports).
**ملاحظة:** بعض الموديولات الثقيلة (XGBoost, LightGBM, TensorFlow) اختيارية وتتطلب تثبيت مكتبات إضافية.
""")

st.sidebar.header("⚙️ إعدادات عامة")
uploaded_file = st.sidebar.file_uploader("📥 ارفع CSV (أو اترك ليستخدم الملف المحلي 'mental_health_lifestyle.csv')", type=['csv'])
use_sample = st.sidebar.checkbox("استخدم ملف المثال المحلي إذا متاح", value=True)
seed = st.sidebar.number_input("Random Seed", value=42, step=1)
np.random.seed(seed)


@st.cache_data
def load_csv(path):
    return pd.read_csv(path)

DATA_PATH = "mental_health_lifestyle.csv"
if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        st.success("تم تحميل الملف المرفوع.")
    except Exception as e:
        st.error(f"فشل تحميل الملف: {e}")
        st.stop()
else:
    try:
        if use_sample:
            df = load_csv(DATA_PATH)
            st.info(f"تم تحميل الملف المحلي: {DATA_PATH}")
        else:
            st.warning("لم يتم رفع ملف. اختاري 'Use sample' أو ارفع ملف CSV.")
            st.stop()
    except Exception as e:
        st.error(f"لم أجد الملف المحلي '{DATA_PATH}'. ارفعي CSV أو أضعي الملف في نفس المجلد. الخطأ: {e}")
        st.stop()

st.sidebar.write(f"Rows: {df.shape[0]} | Columns: {df.shape[1]}")


st.subheader("1) معاينة وتنظيف سريع")
with st.expander("عرض أول صفوف وملخص"):
    st.dataframe(df.head(10))
    st.write("الأعمدة:", list(df.columns))
    st.write("نوع كل عمود:")
    st.write(df.dtypes)

if st.sidebar.checkbox("حذف القيم المفقودة (dropna)"):
    before = df.shape[0]
    df = df.dropna()
    st.sidebar.write(f"تم حذف {before - df.shape[0]} صفوف (NaN)")

text_columns = df.select_dtypes(include=['object']).columns.tolist()

st.subheader("2) تحليل استكشافي متقدم (EDA)")
eda_tab1, eda_tab2, eda_tab3 = st.tabs(["Distributions", "Correlations & Pairplot", "Text Overview"])

with eda_tab1:
    st.write("### توزيعات المتغيرات")
    col = st.selectbox("اختر عمود لعرض التوزيع (Histogram)", df.columns, key="hist_col")
    fig = px.histogram(df, x=col, nbins=50, title=f"Distribution of {col}")
    st.plotly_chart(fig, use_container_width=True)
    if pd.api.types.is_numeric_dtype(df[col]):
        st.write(df[col].describe())

    st.write("---")
    st.write("Boxplot (اختياري لعمود رقمي)")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        bcol = st.selectbox("اختر عمود رقمي لBoxplot", numeric_cols, key="box_col")
        fig2, ax2 = plt.subplots()
        sns.boxplot(y=df[bcol], ax=ax2)
        st.pyplot(fig2)

with eda_tab2:
    st.write("### مصفوفة الارتباط (Heatmap)")
    corr_method = st.selectbox("Method for correlation", ["pearson", "spearman", "kendall"])
    corr = df.corr(method=corr_method)
    fig3, ax3 = plt.subplots(figsize=(12,8))
    sns.heatmap(corr, cmap="coolwarm", annot=False, ax=ax3)
    st.pyplot(fig3)
    if st.checkbox("عرض Pairplot (عينة صغيرة — ثقيل)"):
        try:
            sample_size = st.number_input("Sample size for pairplot (max 2000)", 100, 2000, 500)
            sample = df.select_dtypes(include=[np.number]).sample(min(sample_size, len(df))).dropna()
            pp = sns.pairplot(sample)
            st.pyplot(pp)
        except Exception as e:
            st.error(f"Pairplot failed: {e}")

with eda_tab3:
    st.write("### نظرة على الأعمدة النصية (Text columns)")
    if text_columns:
        tcol = st.selectbox("اختر عمود نصي لاستعراض أو لتحليل النص", text_columns)
        st.write("أكثر 10 كلمات شيوعًا (تقريبًا):")
        sample_text = " ".join(df[tcol].dropna().astype(str).sample(min(5000, df.shape[0]), random_state=seed).tolist())
        # very simple tokenization
        tokens = [w.strip().lower() for w in sample_text.split() if len(w)>2]
        freq = pd.Series(tokens).value_counts().head(30)
        st.bar_chart(freq)
    else:
        st.info("لا أعمدة نصية في الداتا.")


st.subheader("3) تجهيز الميزات (Feature Engineering & Preprocessing)")

target_col = st.selectbox("اختر العمود الهدف (Target variable)", df.columns, index=len(df.columns)-1)
st.write("Target selected:", target_col)

all_features = [c for c in df.columns if c != target_col]
chosen_features = st.multiselect("اختر الأعمدة (Features) لاستخدامها في النمذجة", all_features, default=all_features)

work_df = df[chosen_features + [target_col]].copy()

cat_cols = work_df.select_dtypes(include=['object']).columns.tolist()
st.write("Categorical columns detected:", cat_cols)

le_dict = {}
if st.checkbox("تطبيق Label Encoding للأعمدة النصية تلقائياً", value=True):
    for c in cat_cols:
        le = LabelEncoder()
        try:
            work_df[c] = le.fit_transform(work_df[c].astype(str))
            le_dict[c] = le
        except Exception as e:
            st.warning(f"Failed to encode {c}: {e}")

# fill NA
if work_df.isnull().sum().sum() > 0:
    if st.checkbox("Fill numeric NaN بـ median", value=True):
        for cn in work_df.select_dtypes(include=[np.number]).columns:
            work_df[cn] = work_df[cn].fillna(work_df[cn].median())
    if st.checkbox("Fill categorical NaN بـ 'missing'"):
        for cc in work_df.select_dtypes(exclude=[np.number]).columns:
            work_df[cc] = work_df[cc].fillna("missing")

scale = st.checkbox("تطبيق StandardScaler على الميزات", value=True)
if scale:
    scaler = StandardScaler()
    numeric_features = work_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [f for f in numeric_features if f != target_col]
    work_df[numeric_features] = scaler.fit_transform(work_df[numeric_features])

test_size = st.slider("Test size (%)", 10, 40, 20)
X = work_df.drop(columns=[target_col])
y = work_df[target_col]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size/100.0, random_state=seed, stratify=(y if len(np.unique(y))>1 else None))

st.write("Train shape:", X_train.shape, "Test shape:", X_test.shape)


st.subheader("4) النمذجة — عدة خوارزميات + tuning")

base_models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=seed),
    "GradientBoosting": GradientBoostingClassifier(n_estimators=150, random_state=seed),
    "AdaBoost": AdaBoostClassifier(n_estimators=100, random_state=seed),
    "ExtraTrees": ExtraTreesClassifier(n_estimators=150, random_state=seed),
    "DecisionTree": DecisionTreeClassifier(random_state=seed),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "SVM": SVC(probability=True, kernel='rbf'),
    "NaiveBayes": GaussianNB()
}

if xgb_available:
    base_models["XGBoost"] = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=seed)
else:
    st.sidebar.info("XGBoost غير مُثبت — لتشغيل XGBoost ثبت المكتبة.")

if lgb_available:
    base_models["LightGBM"] = lgb.LGBMClassifier(random_state=seed)
else:
    st.sidebar.info("LightGBM غير مُثبت (اختياري).")

selected = st.multiselect("اختر الموديلات للتدريب والمقارنة", list(base_models.keys()), default=list(base_models.keys())[:5])

do_hyper = st.checkbox("تشغيل Hyperparameter Tuning (RandomizedSearch للوقفة الأولى) — قد يكون بطيئًا", value=False)

results = {}
trained_models = {}

if st.button("🏃 تدريب & تقييم النماذج"):
    progress = st.progress(0)
    total = len(selected)
    for i, name in enumerate(selected):
        st.write(f"---\n**Training {name}...**")
        model = base_models[name]
        if do_hyper:
            param_distributions = {}
            if "Forest" in name or "Trees" in name:
                param_distributions = {
                    "n_estimators": [100, 200, 300],
                    "max_depth": [None, 10, 20, 30],
                    "min_samples_split": [2,5,10]
                }
            elif name == "LogisticRegression":
                param_distributions = {"C": [0.01, 0.1, 1, 10]}
            elif name == "KNN":
                param_distributions = {"n_neighbors": [3,5,7,9]}
            elif name == "SVM":
                param_distributions = {"C": [0.1,1,10], "gamma": ["scale","auto"]}
            else:
                param_distributions = {}

            if param_distributions:
                try:
                    rs = RandomizedSearchCV(model, param_distributions, n_iter=6, cv=3, scoring='accuracy', random_state=seed, n_jobs=-1)
                    rs.fit(X_train, y_train)
                    best_model = rs.best_estimator_
                    st.write(f"Best params for {name}: {rs.best_params_}")
                    model = best_model
                except Exception as e:
                    st.warning(f"Tuning failed for {name}: {e}")

        # fit final
        try:
            model.fit(X_train, y_train)
            preds = model.predict(X_test)
            acc = accuracy_score(y_test, preds)
            f1 = f1_score(y_test, preds, average='weighted') if len(np.unique(y))>1 else None
            results[name] = {"accuracy": acc, "f1": f1}
            trained_models[name] = model
            st.success(f"{name} — accuracy: {acc:.4f}  f1: {f1 if f1 is not None else 'N/A'}")
            st.text(classification_report(y_test, preds))
            # confusion
            fig, ax = plt.subplots()
            sns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt='d', cmap='Blues', ax=ax)
            ax.set_title(f"Confusion Matrix - {name}")
            st.pyplot(fig)
        except Exception as e:
            st.error(f"Training failed for {name}: {e}")

        progress.progress((i+1)/total)

    st.write("### ملخص النتائج")
    if results:
        res_df = pd.DataFrame(results).T
        st.dataframe(res_df.sort_values("accuracy", ascending=False))
    else:
        st.info("لا نتائج بعد — تأكد من اختيار موديلات.")
st.subheader("5) الشبكات العصبية (Neural Networks) — Tabular + NLP")
if st.checkbox("تشغيل MLP (شبكة عصبية لبيانات الجدولية)"):
    if not tf_available:
        st.error("TensorFlow غير مُثبت. لتشغيل الشبكة العصبية ثبت tensorflow.")
    else:
        try:
            input_dim = X_train.shape[1]
            mlp = Sequential([
                Dense(256, activation='relu', input_shape=(input_dim,)),
                Dropout(0.3),
                Dense(128, activation='relu'),
                Dropout(0.2),
                Dense(64, activation='relu'),
                Dense(1, activation='sigmoid')
            ])
            mlp.compile(optimizer='adam', loss='binary_crossentropy' if len(np.unique(y_train))==2 else 'sparse_categorical_crossentropy', metrics=['accuracy'])
            epochs = st.number_input("epochs for MLP", 1, 100, 10)
            batch = st.number_input("batch size", 8, 512, 32)
            hist = mlp.fit(X_train, y_train, validation_split=0.15, epochs=epochs, batch_size=batch, verbose=0)
            loss, acc = mlp.evaluate(X_test, y_test, verbose=0)
            st.success(f"MLP Test Accuracy: {acc:.4f}")
            # plot train/val accuracy
            fig, ax = plt.subplots()
            ax.plot(hist.history.get('accuracy', []), label='train_acc')
            ax.plot(hist.history.get('val_accuracy', []), label='val_acc')
            ax.legend(); ax.set_title("MLP accuracy")
            st.pyplot(fig)
            trained_models['MLP'] = mlp
        except Exception as e:
            st.error(f"MLP training failed: {e}")

if text_columns:
    st.write("----")
    st.write("### NLP: تحويل النصوص + بناء نماذج نصية")
    text_col_for_nlp = st.selectbox("اختر عمود نصي لاستخدامه في نماذج الـ NLP (TF-IDF / Deep models)", text_columns, index=0)

    text_df = df[[text_col_for_nlp, target_col]].dropna()
    st.write("Text examples:")
    st.write(text_df.head(3))

    if st.checkbox("TF-IDF + classical (Logistic / SVM / RF)"):
        ngram = st.selectbox("ngram range", [(1,1),(1,2)], index=0)
        max_features = st.number_input("max_features TF-IDF", 1000, 50000, 5000)
        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram)
        X_text = vectorizer.fit_transform(text_df[text_col_for_nlp].astype(str))
        y_text = LabelEncoder().fit_transform(text_df[target_col])
        Xtr, Xte, ytr, yte = train_test_split(X_text, y_text, test_size=0.2, random_state=seed, stratify=y_text)
        clf_choice = st.selectbox("Classifier for TF-IDF", ["LogisticRegression","SVM","RandomForest"], index=0)
        if st.button("Train TF-IDF model"):
            if clf_choice == "LogisticRegression":
                clf = LogisticRegression(max_iter=1000)
            elif clf_choice == "SVM":
                clf = SVC(probability=True)
            else:
                clf = RandomForestClassifier(n_estimators=200)
            clf.fit(Xtr, ytr)
            preds = clf.predict(Xte)
            st.success(f"TF-IDF model accuracy: {accuracy_score(yte,preds):.4f}")
            st.text(classification_report(yte, preds))
            trained_models['tfidf_'+clf_choice] = (vectorizer, clf)

    if st.checkbox("Deep NLP (Tokenizer + Embedding + LSTM/CNN) — requires TensorFlow"):
        if not tf_available:
            st.error("TensorFlow غير مثبت.")
        else:
            max_words = st.number_input("max_words (vocab size)", 1000, 50000, 10000)
            max_len = st.number_input("max sequence length", 20, 500, 100)
            tokenizer = Tokenizer(num_words=max_words)
            tokenizer.fit_on_texts(text_df[text_col_for_nlp].astype(str))
            sequences = tokenizer.texts_to_sequences(text_df[text_col_for_nlp].astype(str))
            X_seq = pad_sequences(sequences, maxlen=max_len, padding='post')
            y_seq = LabelEncoder().fit_transform(text_df[target_col])

            model_type = st.selectbox("Deep model type", ["LSTM", "CNN", "BiLSTM"])
            if st.button("Train Deep NLP model"):
                try:
                    y_classes = len(np.unique(y_seq))
                    emb_dim = st.number_input("embedding dim", 16, 300, 100)
                    model_nlp = Sequential()
                    model_nlp.add(Embedding(input_dim=max_words, output_dim=emb_dim, input_length=max_len))
                    if model_type == "LSTM":
                        model_nlp.add(LSTM(128))
                    elif model_type == "BiLSTM":
                        model_nlp.add(Bidirectional(LSTM(128)))
                    else:
                        model_nlp.add(Conv1D(128, 5, activation='relu'))
                        model_nlp.add(GlobalMaxPooling1D())
                    model_nlp.add(Dropout(0.3))
                    if y_classes == 2:
                        model_nlp.add(Dense(1, activation='sigmoid'))
                        loss = 'binary_crossentropy'
                    else:
                        model_nlp.add(Dense(y_classes, activation='softmax'))
                        loss = 'sparse_categorical_crossentropy'
                    model_nlp.compile(optimizer='adam', loss=loss, metrics=['accuracy'])
                    Xtr, Xte, ytr, yte = train_test_split(X_seq, y_seq, test_size=0.2, random_state=seed, stratify=y_seq)
                    history = model_nlp.fit(Xtr, ytr, epochs=5, batch_size=64, validation_split=0.1, verbose=0)
                    loss_val, acc_val = model_nlp.evaluate(Xte, yte, verbose=0)
                    st.success(f"Deep NLP test accuracy: {acc_val:.4f}")
                    fig, ax = plt.subplots()
                    ax.plot(history.history.get('accuracy',[]), label='train_acc')
                    ax.plot(history.history.get('val_accuracy',[]), label='val_acc')
                    ax.legend(); st.pyplot(fig)
                    trained_models['deep_nlp'] = (tokenizer, model_nlp)
                except Exception as e:
                    st.error(f"Deep NLP training failed: {e}")

st.subheader("6) التحليل غير الخاضع للإشراف (Clustering + PCA)")

if st.checkbox("تشغيل KMeans + PCA لتصور التجمعات"):
    try:
        n_clusters = st.number_input("عدد التجمعات (K)", 2, 15, 4)
        km = KMeans(n_clusters=n_clusters, random_state=seed)
        km.fit(X)
        labels_k = km.predict(X)
        pca = PCA(n_components=2)
        pcs = pca.fit_transform(X)
        pc_df = pd.DataFrame(pcs, columns=['PC1','PC2'])
        pc_df['cluster'] = labels_k
        fig = px.scatter(pc_df, x='PC1', y='PC2', color='cluster', title='KMeans clusters (PCA 2D)')
        st.plotly_chart(fig, use_container_width=True)
        st.write("Cluster counts:")
        st.write(pd.Series(labels_k).value_counts())
    except Exception as e:
        st.error(f"KMeans/PCA failed: {e}")


st.subheader("7) Prediction UI — تنبؤ باستخدام موديل مدرّب")

if trained_models:
    model_names = list(trained_models.keys())
    chosen_for_pred = st.selectbox("اختر نموذج للتنبؤ من النماذج المدربة", model_names)
    if st.button("استخدام النموذج للتنبؤ من إدخال يدوي"):
        sample_input = {}
        for col in X.columns:
            if col in cat_cols:
                vals = df[col].unique().tolist()
                if len(vals) <= 20:
                    sample_input[col] = st.selectbox(f"{col}", vals, key=f"inp_{col}")
                else:
                    sample_input[col] = st.number_input(f"{col}", float(df[col].min()), float(df[col].max()), float(df[col].median()))
            else:
                sample_input[col] = st.number_input(f"{col}", float(df[col].min()), float(df[col].max()), float(df[col].median()))
        sin = pd.DataFrame([sample_input])
        for c, le in le_dict.items():
            if c in sin.columns:
                try:
                    sin[c] = le.transform(sin[c].astype(str))
                except Exception:
                    pass
        if scale:
            sin[numeric_features] = scaler.transform(sin[numeric_features])
        model_obj = trained_models[chosen_for_pred]
        if isinstance(model_obj, tuple) and len(model_obj) == 2 and hasattr(model_obj[1], "predict"):
            vec, clf = model_obj
            Xsin = vec.transform(sin[text_col_for_nlp].astype(str))
            pred = clf.predict(Xsin)
        elif tf_available and hasattr(model_obj, 'predict') and 'keras' in str(type(model_obj)).lower():
            pred = model_obj.predict(sin)  # may need reshape
        else:
            pred = model_obj.predict(sin)
        st.success(f"Prediction: {pred}")

else:
    st.info("لا توجد نماذج مدربة بعد — درّب أولا بعض النماذج في القسم (4) أو (5).")

st.subheader("8) حفظ/تحميل النماذج والتقارير")
if trained_models:
    save_name = st.text_input("اسم الملف لحفظ النموذج (بدون امتداد)", value=f"mh_model_{datetime.now().strftime('%Y%m%d_%H%M')}")
    model_to_save = st.selectbox("اختر نموذج لحفظه", list(trained_models.keys()))
    if st.button("🔁 حفظ النموذج كـ .joblib"):
        try:
            obj = trained_models[model_to_save]
د            if tf_available and 'keras' in str(type(obj)).lower():
                tmp_path = f"{save_name}_{model_to_save}.h5"
                obj.save(tmp_path)
                with open(tmp_path,"rb") as f:
                    st.download_button(f"Download {tmp_path}", f, file_name=tmp_path)
            else:
                buf = BytesIO()
                joblib.dump(obj, buf)
                buf.seek(0)
                st.download_button(f"Download {save_name}_{model_to_save}.joblib", data=buf, file_name=f"{save_name}_{model_to_save}.joblib")
                st.success("تم إنشاء ملف التحميل.")
        except Exception as e:
            st.error(f"حفظ النموذج فشل: {e}")

def gen_simple_report(results_dict):
    from reportlab.lib.pagesizes import letter
    from reportlab.pdfgen import canvas
    buf = BytesIO()
    c = canvas.Canvas(buf, pagesize=letter)
    c.setFont("Helvetica", 12)
    c.drawString(50, 750, "Mental Health & Lifestyle Analysis Report")
    c.drawString(50, 730, f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")
    c.drawString(50, 710, f"Target: {target_col}")
    y = 690
    for k,v in (results_dict.items() if results_dict else []):
        c.drawString(50, y, f"{k}: {v}")
        y -= 20
    c.showPage()
    c.save()
    buf.seek(0)
    return buf

if st.button("📄 تنزيل تقرير سريع PDF"):
    pdf_buf = gen_simple_report({k: v for k,v in results.items()}) if 'results' in locals() else gen_simple_report({})
    st.download_button("Download Report PDF", data=pdf_buf, file_name="MH_report.pdf", mime="application/pdf")


st.markdown("---")
st.markdown("**ملاحظات نهائية:**")
st.markdown("""
- لتشغيل كل الميزات (XGBoost, LightGBM, TensorFlow, TextBlob) ثبت المكتبات في `requirements.txt`.
- بعض الأجزاء (Pairplot، تدريب Deep NLP) يمكن أن تكون ثقيلة على موارد محدودة — استخدمي Sample أو خففي عدد epochs.
- هذا التطبيق مرن: يمكنك تعليق/إلغاء تفعيل أقسام حسب الحاجة.
""")



import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from textblob import TextBlob
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

st.set_page_config(page_title="Mental Health and Lifestyle AI Dashboard", layout="wide")
st.title("Mental Health and Lifestyle Analysis with AI")

@st.cache_data
def load_data():
    df = pd.read_csv("mental_health_lifestyle.csv")
    return df

df = load_data()

st.sidebar.title("Dashboard Control")
if st.sidebar.checkbox("Show first 10 rows"):
    st.dataframe(df.head(10))

st.sidebar.write("Number of rows:", df.shape[0])
st.sidebar.write("Number of columns:", df.shape[1])

col1, col2 = st.columns(2)

with col1:
    st.subheader("Distribution of Target Column")
    target_col = st.selectbox("Select Target Column", df.columns)
    fig = px.histogram(df, x=target_col, color=target_col)
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.subheader("Correlation Heatmap")
    numeric_df = df.select_dtypes(include=['float64', 'int64'])
    fig, ax = plt.subplots(figsize=(7, 5))
    sns.heatmap(numeric_df.corr(), cmap="coolwarm", ax=ax)
    st.pyplot(fig)

df = df.dropna()
label_cols = df.select_dtypes(include=['object']).columns
encoder = LabelEncoder()
for col in label_cols:
    df[col] = encoder.fit_transform(df[col])

target = target_col
X = df.drop(columns=[target])
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

st.header("Model Training and Evaluation")

models = {
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "Support Vector Machine": SVC(kernel="rbf", probability=True),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(random_state=42)
}

selected_models = st.multiselect("Select models to train", list(models.keys()), default=["Random Forest", "Logistic Regression"])

results = {}

if st.button("Train Selected Models"):
    for name in selected_models:
        model = models[name]
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        acc = accuracy_score(y_test, preds)
        results[name] = acc

    st.subheader("Model Accuracy Comparison")
    result_df = pd.DataFrame(list(results.items()), columns=["Model", "Accuracy"]).sort_values(by="Accuracy", ascending=False)
    st.dataframe(result_df)

    fig = px.bar(result_df, x="Model", y="Accuracy", color="Accuracy", title="Model Accuracy Comparison")
    st.plotly_chart(fig, use_container_width=True)

    best_model_name = max(results, key=results.get)
    st.success(f"Best performing model: {best_model_name} with accuracy {results[best_model_name]:.2f}")

if st.checkbox("Show Confusion Matrix for Best Model"):
    best_model = models[best_model_name]
    preds = best_model.predict(X_test)
    cm = confusion_matrix(y_test, preds)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    st.pyplot(fig)

st.subheader("Neural Network Model")

if st.button("Train Neural Network"):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    nn_model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    history = nn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)

    test_loss, test_acc = nn_model.evaluate(X_test_scaled, y_test, verbose=0)
    st.success(f"Neural Network Test Accuracy: {test_acc:.2f}")

    fig, ax = plt.subplots(1, 2, figsize=(10, 4))
    ax[0].plot(history.history['accuracy'], label='Train Accuracy')
    ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    ax[0].legend()
    ax[0].set_title("Accuracy Over Epochs")

    ax[1].plot(history.history['loss'], label='Train Loss')
    ax[1].plot(history.history['val_loss'], label='Validation Loss')
    ax[1].legend()
    ax[1].set_title("Loss Over Epochs")

    st.pyplot(fig)
st.header("Natural Language Processing (NLP) and Sentiment Analysis")

text_input = st.text_area("Enter a sentence or short paragraph to analyze its sentiment:")
if st.button("Analyze Sentiment"):
    if text_input.strip() != "":
        sentiment = TextBlob(text_input).sentiment.polarity
        if sentiment > 0:
            st.success(f"Positive Sentiment ({sentiment:.2f})")
        elif sentiment < 0:
            st.error(f"Negative Sentiment ({sentiment:.2f})")
        else:
            st.warning("Neutral Sentiment (0.00)")
    else:
        st.warning("Please enter some text.")

st.header("Clustering and Data Patterns (K-Means + PCA)")

num_clusters = st.slider("Select number of clusters", 2, 10, 3)
scaled = StandardScaler().fit_transform(X)
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
labels = kmeans.fit_predict(scaled)
df['Cluster'] = labels

pca = PCA(2)
components = pca.fit_transform(scaled)
pca_df = pd.DataFrame(data=components, columns=["PC1", "PC2"])
pca_df['Cluster'] = labels

fig = px.scatter(pca_df, x="PC1", y="PC2", color=pca_df["Cluster"].astype(str),
                 title="K-Means Clustering with PCA Visualization")
st.plotly_chart(fig, use_container_width=True)

st.header("Make Predictions")

user_input = {}
for col in X.columns:
    val = st.number_input(f"Enter value for {col}", value=float(df[col].mean()))
    user_input[col] = val

user_df = pd.DataFrame([user_input])

if st.button("Predict Using Best Model"):
    best_model = models[max(results, key=results.get)]
    prediction = best_model.predict(user_df)[0]
    st.success(f"Predicted Class: {prediction}")

st.header("Summary Insights")

col1, col2 = st.columns(2)
with col1:
    st.subheader("Top Correlations")
    corr = df.corr()[target].sort_values(ascending=False)
    st.write(corr.head(10))

with col2:
    st.subheader("Feature Importance (Random Forest)")
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X_train, y_train)
    importances = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf.feature_importances_
    }).sort_values(by='Importance', ascending=False)
    fig = px.bar(importances.head(10), x='Feature', y='Importance', title='Top 10 Important Features')
    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")
st.markdown("**Developed for AI Mental Health & Lifestyle Research Dashboard**")

