# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KwNvsGvzTcJDQjcu9lWIq8kzQBxDa6Qc
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import io
import joblib
from datetime import datetime
from io import BytesIO
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

try:
    from xgboost import XGBClassifier
    xgb_available = True
except Exception:
    xgb_available = False

try:
    import lightgbm as lgb
    lgb_available = True
except Exception:
    lgb_available = False

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    tf_available = True
except Exception:
    tf_available = False

try:
    from textblob import TextBlob
    textblob_available = True
except Exception:
    textblob_available = False

st.set_page_config(page_title=" AI Mental Health Intelligence (Advanced)", layout="wide", page_icon="ğŸ§©")
st.title(" AI-Powered Mental Health & Lifestyle Intelligence (Advanced)")

st.markdown("""
**Overview:** Ù†Ø¸Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„ Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ­Ø© Ø§Ù„Ù†ÙØ³ÙŠØ© ÙˆØ§Ù„Ø³Ù„ÙˆÙƒÙŠØ© (EDA + ML + DL + NLP + Clustering + Prediction + Reports).
**Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ¯ÙŠÙˆÙ„Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© (XGBoost, LightGBM, TensorFlow) Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© ÙˆØªØªØ·Ù„Ø¨ ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©.
""")

st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©")
uploaded_file = st.sidebar.file_uploader("ğŸ“¥ Ø§Ø±ÙØ¹ CSV (Ø£Ùˆ Ø§ØªØ±Ùƒ Ù„ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ 'mental_health_lifestyle.csv')", type=['csv'])
use_sample = st.sidebar.checkbox("Ø§Ø³ØªØ®Ø¯Ù… Ù…Ù„Ù Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¥Ø°Ø§ Ù…ØªØ§Ø­", value=True)
seed = st.sidebar.number_input("Random Seed", value=42, step=1)
np.random.seed(seed)


@st.cache_data
def load_csv(path):
    return pd.read_csv(path)

DATA_PATH = "mental_health_lifestyle.csv"
if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹.")
    except Exception as e:
        st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {e}")
        st.stop()
else:
    try:
        if use_sample:
            df = load_csv(DATA_PATH)
            st.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ: {DATA_PATH}")
        else:
            st.warning("Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ù…Ù„Ù. Ø§Ø®ØªØ§Ø±ÙŠ 'Use sample' Ø£Ùˆ Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV.")
            st.stop()
    except Exception as e:
        st.error(f"Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ '{DATA_PATH}'. Ø§Ø±ÙØ¹ÙŠ CSV Ø£Ùˆ Ø£Ø¶Ø¹ÙŠ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯. Ø§Ù„Ø®Ø·Ø£: {e}")
        st.stop()

st.sidebar.write(f"Rows: {df.shape[0]} | Columns: {df.shape[1]}")


st.subheader("1) Ù…Ø¹Ø§ÙŠÙ†Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø³Ø±ÙŠØ¹")
with st.expander("Ø¹Ø±Ø¶ Ø£ÙˆÙ„ ØµÙÙˆÙ ÙˆÙ…Ù„Ø®Øµ"):
    st.dataframe(df.head(10))
    st.write("Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:", list(df.columns))
    st.write("Ù†ÙˆØ¹ ÙƒÙ„ Ø¹Ù…ÙˆØ¯:")
    st.write(df.dtypes)

if st.sidebar.checkbox("Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (dropna)"):
    before = df.shape[0]
    df = df.dropna()
    st.sidebar.write(f"ØªÙ… Ø­Ø°Ù {before - df.shape[0]} ØµÙÙˆÙ (NaN)")

text_columns = df.select_dtypes(include=['object']).columns.tolist()

st.subheader("2) ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ Ù…ØªÙ‚Ø¯Ù… (EDA)")
eda_tab1, eda_tab2, eda_tab3 = st.tabs(["Distributions", "Correlations & Pairplot", "Text Overview"])

with eda_tab1:
    st.write("### ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª")
    col = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ²ÙŠØ¹ (Histogram)", df.columns, key="hist_col")
    fig = px.histogram(df, x=col, nbins=50, title=f"Distribution of {col}")
    st.plotly_chart(fig, use_container_width=True)
    if pd.api.types.is_numeric_dtype(df[col]):
        st.write(df[col].describe())

    st.write("---")
    st.write("Boxplot (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù…ÙŠ)")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        bcol = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù…ÙŠ Ù„Boxplot", numeric_cols, key="box_col")
        fig2, ax2 = plt.subplots()
        sns.boxplot(y=df[bcol], ax=ax2)
        st.pyplot(fig2)

with eda_tab2:
    st.write("### Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· (Heatmap)")
    corr_method = st.selectbox("Method for correlation", ["pearson", "spearman", "kendall"])
    corr = df.corr(method=corr_method)
    fig3, ax3 = plt.subplots(figsize=(12,8))
    sns.heatmap(corr, cmap="coolwarm", annot=False, ax=ax3)
    st.pyplot(fig3)
    if st.checkbox("Ø¹Ø±Ø¶ Pairplot (Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© â€” Ø«Ù‚ÙŠÙ„)"):
        try:
            sample_size = st.number_input("Sample size for pairplot (max 2000)", 100, 2000, 500)
            sample = df.select_dtypes(include=[np.number]).sample(min(sample_size, len(df))).dropna()
            pp = sns.pairplot(sample)
            st.pyplot(pp)
        except Exception as e:
            st.error(f"Pairplot failed: {e}")

with eda_tab3:
    st.write("### Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© (Text columns)")
    if text_columns:
        tcol = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ Ù†ØµÙŠ Ù„Ø§Ø³ØªØ¹Ø±Ø§Ø¶ Ø£Ùˆ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ", text_columns)
        st.write("Ø£ÙƒØ«Ø± 10 ÙƒÙ„Ù…Ø§Øª Ø´ÙŠÙˆØ¹Ù‹Ø§ (ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§):")
        sample_text = " ".join(df[tcol].dropna().astype(str).sample(min(5000, df.shape[0]), random_state=seed).tolist())
        # very simple tokenization
        tokens = [w.strip().lower() for w in sample_text.split() if len(w)>2]
        freq = pd.Series(tokens).value_counts().head(30)
        st.bar_chart(freq)
    else:
        st.info("Ù„Ø§ Ø£Ø¹Ù…Ø¯Ø© Ù†ØµÙŠØ© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§.")


st.subheader("3) ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…ÙŠØ²Ø§Øª (Feature Engineering & Preprocessing)")

target_col = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù (Target variable)", df.columns, index=len(df.columns)-1)
st.write("Target selected:", target_col)

all_features = [c for c in df.columns if c != target_col]
chosen_features = st.multiselect("Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Features) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ù†Ù…Ø°Ø¬Ø©", all_features, default=all_features)

work_df = df[chosen_features + [target_col]].copy()

cat_cols = work_df.select_dtypes(include=['object']).columns.tolist()
st.write("Categorical columns detected:", cat_cols)

le_dict = {}
if st.checkbox("ØªØ·Ø¨ÙŠÙ‚ Label Encoding Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹", value=True):
    for c in cat_cols:
        le = LabelEncoder()
        try:
            work_df[c] = le.fit_transform(work_df[c].astype(str))
            le_dict[c] = le
        except Exception as e:
            st.warning(f"Failed to encode {c}: {e}")

# fill NA
if work_df.isnull().sum().sum() > 0:
    if st.checkbox("Fill numeric NaN Ø¨Ù€ median", value=True):
        for cn in work_df.select_dtypes(include=[np.number]).columns:
            work_df[cn] = work_df[cn].fillna(work_df[cn].median())
    if st.checkbox("Fill categorical NaN Ø¨Ù€ 'missing'"):
        for cc in work_df.select_dtypes(exclude=[np.number]).columns:
            work_df[cc] = work_df[cc].fillna("missing")

scale = st.checkbox("ØªØ·Ø¨ÙŠÙ‚ StandardScaler Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª", value=True)
if scale:
    scaler = StandardScaler()
    numeric_features = work_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [f for f in numeric_features if f != target_col]
    work_df[numeric_features] = scaler.fit_transform(work_df[numeric_features])

test_size = st.slider("Test size (%)", 10, 40, 20)
X = work_df.drop(columns=[target_col])
y = work_df[target_col]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size/100.0, random_state=seed, stratify=(y if len(np.unique(y))>1 else None))

st.write("Train shape:", X_train.shape, "Test shape:", X_test.shape)


st.subheader("4) Ø§Ù„Ù†Ù…Ø°Ø¬Ø© â€” Ø¹Ø¯Ø© Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª + tuning")

base_models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=seed),
    "GradientBoosting": GradientBoostingClassifier(n_estimators=150, random_state=seed),
    "AdaBoost": AdaBoostClassifier(n_estimators=100, random_state=seed),
    "ExtraTrees": ExtraTreesClassifier(n_estimators=150, random_state=seed),
    "DecisionTree": DecisionTreeClassifier(random_state=seed),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "SVM": SVC(probability=True, kernel='rbf'),
    "NaiveBayes": GaussianNB()
}

if xgb_available:
    base_models["XGBoost"] = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=seed)
else:
    st.sidebar.info("XGBoost ØºÙŠØ± Ù…ÙØ«Ø¨Øª â€” Ù„ØªØ´ØºÙŠÙ„ XGBoost Ø«Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø©.")

if lgb_available:
    base_models["LightGBM"] = lgb.LGBMClassifier(random_state=seed)
else:
    st.sidebar.info("LightGBM ØºÙŠØ± Ù…ÙØ«Ø¨Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ).")

selected = st.multiselect("Ø§Ø®ØªØ± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©", list(base_models.keys()), default=list(base_models.keys())[:5])

do_hyper = st.checkbox("ØªØ´ØºÙŠÙ„ Hyperparameter Tuning (RandomizedSearch Ù„Ù„ÙˆÙ‚ÙØ© Ø§Ù„Ø£ÙˆÙ„Ù‰) â€” Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø·ÙŠØ¦Ù‹Ø§", value=False)

results = {}
trained_models = {}

if st.button("ğŸƒ ØªØ¯Ø±ÙŠØ¨ & ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"):
    progress = st.progress(0)
    total = len(selected)
    for i, name in enumerate(selected):
        st.write(f"---\n**Training {name}...**")
        model = base_models[name]
        if do_hyper:
            param_distributions = {}
            if "Forest" in name or "Trees" in name:
                param_distributions = {
                    "n_estimators": [100, 200, 300],
                    "max_depth": [None, 10, 20, 30],
                    "min_samples_split": [2,5,10]
                }
            elif name == "LogisticRegression":
                param_distributions = {"C": [0.01, 0.1, 1, 10]}
            elif name == "KNN":
                param_distributions = {"n_neighbors": [3,5,7,9]}
            elif name == "SVM":
                param_distributions = {"C": [0.1,1,10], "gamma": ["scale","auto"]}
            else:
                param_distributions = {}

            if param_distributions:
                try:
                    rs = RandomizedSearchCV(model, param_distributions, n_iter=6, cv=3, scoring='accuracy', random_state=seed, n_jobs=-1)
                    rs.fit(X_train, y_train)
                    best_model = rs.best_estimator_
                    st.write(f"Best params for {name}: {rs.best_params_}")
                    model = best_model
                except Exception as e:
                    st.warning(f"Tuning failed for {name}: {e}")

        # fit final
        try:
            model.fit(X_train, y_train)
            preds = model.predict(X_test)
            acc = accuracy_score(y_test, preds)
            f1 = f1_score(y_test, preds, average='weighted') if len(np.unique(y))>1 else None
            results[name] = {"accuracy": acc, "f1": f1}
            trained_models[name] = model
            st.success(f"{name} â€” accuracy: {acc:.4f}  f1: {f1 if f1 is not None else 'N/A'}")
            st.text(classification_report(y_test, preds))
            # confusion
            fig, ax = plt.subplots()
            sns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt='d', cmap='Blues', ax=ax)
            ax.set_title(f"Confusion Matrix - {name}")
            st.pyplot(fig)
        except Exception as e:
            st.error(f"Training failed for {name}: {e}")

        progress.progress((i+1)/total)

    st.write("### Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    if results:
        res_df = pd.DataFrame(results).T
        st.dataframe(res_df.sort_values("accuracy", ascending=False))
    else:
        st.info("Ù„Ø§ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø¹Ø¯ â€” ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„Ø§Øª.")
st.subheader("5) Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Neural Networks) â€” Tabular + NLP")
if st.checkbox("ØªØ´ØºÙŠÙ„ MLP (Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©)"):
    if not tf_available:
        st.error("TensorFlow ØºÙŠØ± Ù…ÙØ«Ø¨Øª. Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø«Ø¨Øª tensorflow.")
    else:
        try:
            input_dim = X_train.shape[1]
            mlp = Sequential([
                Dense(256, activation='relu', input_shape=(input_dim,)),
                Dropout(0.3),
                Dense(128, activation='relu'),
                Dropout(0.2),
                Dense(64, activation='relu'),
                Dense(1, activation='sigmoid')
            ])
            mlp.compile(optimizer='adam', loss='binary_crossentropy' if len(np.unique(y_train))==2 else 'sparse_categorical_crossentropy', metrics=['accuracy'])
            epochs = st.number_input("epochs for MLP", 1, 100, 10)
            batch = st.number_input("batch size", 8, 512, 32)
            hist = mlp.fit(X_train, y_train, validation_split=0.15, epochs=epochs, batch_size=batch, verbose=0)
            loss, acc = mlp.evaluate(X_test, y_test, verbose=0)
            st.success(f"MLP Test Accuracy: {acc:.4f}")
            # plot train/val accuracy
            fig, ax = plt.subplots()
            ax.plot(hist.history.get('accuracy', []), label='train_acc')
            ax.plot(hist.history.get('val_accuracy', []), label='val_acc')
            ax.legend(); ax.set_title("MLP accuracy")
            st.pyplot(fig)
            trained_models['MLP'] = mlp
        except Exception as e:
            st.error(f"MLP training failed: {e}")

if text_columns:
    st.write("----")
    st.write("### NLP: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ + Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù†ØµÙŠØ©")
    text_col_for_nlp = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ Ù†ØµÙŠ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù€ NLP (TF-IDF / Deep models)", text_columns, index=0)

    text_df = df[[text_col_for_nlp, target_col]].dropna()
    st.write("Text examples:")
    st.write(text_df.head(3))

    if st.checkbox("TF-IDF + classical (Logistic / SVM / RF)"):
        ngram = st.selectbox("ngram range", [(1,1),(1,2)], index=0)
        max_features = st.number_input("max_features TF-IDF", 1000, 50000, 5000)
        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram)
        X_text = vectorizer.fit_transform(text_df[text_col_for_nlp].astype(str))
        y_text = LabelEncoder().fit_transform(text_df[target_col])
        Xtr, Xte, ytr, yte = train_test_split(X_text, y_text, test_size=0.2, random_state=seed, stratify=y_text)
        clf_choice = st.selectbox("Classifier for TF-IDF", ["LogisticRegression","SVM","RandomForest"], index=0)
        if st.button("Train TF-IDF model"):
            if clf_choice == "LogisticRegression":
                clf = LogisticRegression(max_iter=1000)
            elif clf_choice == "SVM":
                clf = SVC(probability=True)
            else:
                clf = RandomForestClassifier(n_estimators=200)
            clf.fit(Xtr, ytr)
            preds = clf.predict(Xte)
            st.success(f"TF-IDF model accuracy: {accuracy_score(yte,preds):.4f}")
            st.text(classification_report(yte, preds))
            trained_models['tfidf_'+clf_choice] = (vectorizer, clf)

    if st.checkbox("Deep NLP (Tokenizer + Embedding + LSTM/CNN) â€” requires TensorFlow"):
        if not tf_available:
            st.error("TensorFlow ØºÙŠØ± Ù…Ø«Ø¨Øª.")
        else:
            max_words = st.number_input("max_words (vocab size)", 1000, 50000, 10000)
            max_len = st.number_input("max sequence length", 20, 500, 100)
            tokenizer = Tokenizer(num_words=max_words)
            tokenizer.fit_on_texts(text_df[text_col_for_nlp].astype(str))
            sequences = tokenizer.texts_to_sequences(text_df[text_col_for_nlp].astype(str))
            X_seq = pad_sequences(sequences, maxlen=max_len, padding='post')
            y_seq = LabelEncoder().fit_transform(text_df[target_col])

            model_type = st.selectbox("Deep model type", ["LSTM", "CNN", "BiLSTM"])
            if st.button("Train Deep NLP model"):
                try:
                    y_classes = len(np.unique(y_seq))
                    emb_dim = st.number_input("embedding dim", 16, 300, 100)
                    model_nlp = Sequential()
                    model_nlp.add(Embedding(input_dim=max_words, output_dim=emb_dim, input_length=max_len))
                    if model_type == "LSTM":
                        model_nlp.add(LSTM(128))
                    elif model_type == "BiLSTM":
                        model_nlp.add(Bidirectional(LSTM(128)))
                    else:
                        model_nlp.add(Conv1D(128, 5, activation='relu'))
                        model_nlp.add(GlobalMaxPooling1D())
                    model_nlp.add(Dropout(0.3))
                    if y_classes == 2:
                        model_nlp.add(Dense(1, activation='sigmoid'))
                        loss = 'binary_crossentropy'
                    else:
                        model_nlp.add(Dense(y_classes, activation='softmax'))
                        loss = 'sparse_categorical_crossentropy'
                    model_nlp.compile(optimizer='adam', loss=loss, metrics=['accuracy'])
                    Xtr, Xte, ytr, yte = train_test_split(X_seq, y_seq, test_size=0.2, random_state=seed, stratify=y_seq)
                    history = model_nlp.fit(Xtr, ytr, epochs=5, batch_size=64, validation_split=0.1, verbose=0)
                    loss_val, acc_val = model_nlp.evaluate(Xte, yte, verbose=0)
                    st.success(f"Deep NLP test accuracy: {acc_val:.4f}")
                    fig, ax = plt.subplots()
                    ax.plot(history.history.get('accuracy',[]), label='train_acc')
                    ax.plot(history.history.get('val_accuracy',[]), label='val_acc')
                    ax.legend(); st.pyplot(fig)
                    trained_models['deep_nlp'] = (tokenizer, model_nlp)
                except Exception as e:
                    st.error(f"Deep NLP training failed: {e}")

st.subheader("6) Ø§Ù„ØªØ­Ù„ÙŠÙ„ ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù (Clustering + PCA)")

if st.checkbox("ØªØ´ØºÙŠÙ„ KMeans + PCA Ù„ØªØµÙˆØ± Ø§Ù„ØªØ¬Ù…Ø¹Ø§Øª"):
    try:
        n_clusters = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¬Ù…Ø¹Ø§Øª (K)", 2, 15, 4)
        km = KMeans(n_clusters=n_clusters, random_state=seed)
        km.fit(X)
        labels_k = km.predict(X)
        pca = PCA(n_components=2)
        pcs = pca.fit_transform(X)
        pc_df = pd.DataFrame(pcs, columns=['PC1','PC2'])
        pc_df['cluster'] = labels_k
        fig = px.scatter(pc_df, x='PC1', y='PC2', color='cluster', title='KMeans clusters (PCA 2D)')
        st.plotly_chart(fig, use_container_width=True)
        st.write("Cluster counts:")
        st.write(pd.Series(labels_k).value_counts())
    except Exception as e:
        st.error(f"KMeans/PCA failed: {e}")


st.subheader("7) Prediction UI â€” ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¯Ø±Ù‘Ø¨")

if trained_models:
    model_names = list(trained_models.keys())
    chosen_for_pred = st.selectbox("Ø§Ø®ØªØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©", model_names)
    if st.button("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙ†Ø¨Ø¤ Ù…Ù† Ø¥Ø¯Ø®Ø§Ù„ ÙŠØ¯ÙˆÙŠ"):
        sample_input = {}
        for col in X.columns:
            if col in cat_cols:
                vals = df[col].unique().tolist()
                if len(vals) <= 20:
                    sample_input[col] = st.selectbox(f"{col}", vals, key=f"inp_{col}")
                else:
                    sample_input[col] = st.number_input(f"{col}", float(df[col].min()), float(df[col].max()), float(df[col].median()))
            else:
                sample_input[col] = st.number_input(f"{col}", float(df[col].min()), float(df[col].max()), float(df[col].median()))
        sin = pd.DataFrame([sample_input])
        for c, le in le_dict.items():
            if c in sin.columns:
                try:
                    sin[c] = le.transform(sin[c].astype(str))
                except Exception:
                    pass
        if scale:
            sin[numeric_features] = scaler.transform(sin[numeric_features])
        model_obj = trained_models[chosen_for_pred]
        if isinstance(model_obj, tuple) and len(model_obj) == 2 and hasattr(model_obj[1], "predict"):
            vec, clf = model_obj
            Xsin = vec.transform(sin[text_col_for_nlp].astype(str))
            pred = clf.predict(Xsin)
        elif tf_available and hasattr(model_obj, 'predict') and 'keras' in str(type(model_obj)).lower():
            pred = model_obj.predict(sin)  # may need reshape
        else:
            pred = model_obj.predict(sin)
        st.success(f"Prediction: {pred}")

else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø© Ø¨Ø¹Ø¯ â€” Ø¯Ø±Ù‘Ø¨ Ø£ÙˆÙ„Ø§ Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø§Ù„Ù‚Ø³Ù… (4) Ø£Ùˆ (5).")

st.subheader("8) Ø­ÙØ¸/ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„ØªÙ‚Ø§Ø±ÙŠØ±")
if trained_models:
    save_name = st.text_input("Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø¨Ø¯ÙˆÙ† Ø§Ù…ØªØ¯Ø§Ø¯)", value=f"mh_model_{datetime.now().strftime('%Y%m%d_%H%M')}")
    model_to_save = st.selectbox("Ø§Ø®ØªØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ø­ÙØ¸Ù‡", list(trained_models.keys()))
    if st.button("ğŸ” Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ€ .joblib"):
        try:
            obj = trained_models[model_to_save]
Ø¯            if tf_available and 'keras' in str(type(obj)).lower():
                tmp_path = f"{save_name}_{model_to_save}.h5"
                obj.save(tmp_path)
                with open(tmp_path,"rb") as f:
                    st.download_button(f"Download {tmp_path}", f, file_name=tmp_path)
            else:
                buf = BytesIO()
                joblib.dump(obj, buf)
                buf.seek(0)
                st.download_button(f"Download {save_name}_{model_to_save}.joblib", data=buf, file_name=f"{save_name}_{model_to_save}.joblib")
                st.success("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªØ­Ù…ÙŠÙ„.")
        except Exception as e:
            st.error(f"Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙØ´Ù„: {e}")

def gen_simple_report(results_dict):
    from reportlab.lib.pagesizes import letter
    from reportlab.pdfgen import canvas
    buf = BytesIO()
    c = canvas.Canvas(buf, pagesize=letter)
    c.setFont("Helvetica", 12)
    c.drawString(50, 750, "Mental Health & Lifestyle Analysis Report")
    c.drawString(50, 730, f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")
    c.drawString(50, 710, f"Target: {target_col}")
    y = 690
    for k,v in (results_dict.items() if results_dict else []):
        c.drawString(50, y, f"{k}: {v}")
        y -= 20
    c.showPage()
    c.save()
    buf.seek(0)
    return buf

if st.button("ğŸ“„ ØªÙ†Ø²ÙŠÙ„ ØªÙ‚Ø±ÙŠØ± Ø³Ø±ÙŠØ¹ PDF"):
    pdf_buf = gen_simple_report({k: v for k,v in results.items()}) if 'results' in locals() else gen_simple_report({})
    st.download_button("Download Report PDF", data=pdf_buf, file_name="MH_report.pdf", mime="application/pdf")


st.markdown("---")
st.markdown("**Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©:**")
st.markdown("""
- Ù„ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª (XGBoost, LightGBM, TensorFlow, TextBlob) Ø«Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ÙÙŠ `requirements.txt`.
- Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ (PairplotØŒ ØªØ¯Ø±ÙŠØ¨ Deep NLP) ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø«Ù‚ÙŠÙ„Ø© Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø±Ø¯ Ù…Ø­Ø¯ÙˆØ¯Ø© â€” Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Sample Ø£Ùˆ Ø®ÙÙÙŠ Ø¹Ø¯Ø¯ epochs.
- Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…Ø±Ù†: ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ù„ÙŠÙ‚/Ø¥Ù„ØºØ§Ø¡ ØªÙØ¹ÙŠÙ„ Ø£Ù‚Ø³Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©.
""")



import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from textblob import TextBlob
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

st.set_page_config(page_title="Mental Health and Lifestyle AI Dashboard", layout="wide")
st.title("Mental Health and Lifestyle Analysis with AI")

@st.cache_data
def load_data():
    df = pd.read_csv("mental_health_lifestyle.csv")
    return df

df = load_data()

st.sidebar.title("Dashboard Control")
if st.sidebar.checkbox("Show first 10 rows"):
    st.dataframe(df.head(10))

st.sidebar.write("Number of rows:", df.shape[0])
st.sidebar.write("Number of columns:", df.shape[1])

col1, col2 = st.columns(2)

with col1:
    st.subheader("Distribution of Target Column")
    target_col = st.selectbox("Select Target Column", df.columns)
    fig = px.histogram(df, x=target_col, color=target_col)
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.subheader("Correlation Heatmap")
    numeric_df = df.select_dtypes(include=['float64', 'int64'])
    fig, ax = plt.subplots(figsize=(7, 5))
    sns.heatmap(numeric_df.corr(), cmap="coolwarm", ax=ax)
    st.pyplot(fig)

df = df.dropna()
label_cols = df.select_dtypes(include=['object']).columns
encoder = LabelEncoder()
for col in label_cols:
    df[col] = encoder.fit_transform(df[col])

target = target_col
X = df.drop(columns=[target])
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

st.header("Model Training and Evaluation")

models = {
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "Support Vector Machine": SVC(kernel="rbf", probability=True),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(random_state=42)
}

selected_models = st.multiselect("Select models to train", list(models.keys()), default=["Random Forest", "Logistic Regression"])

results = {}

if st.button("Train Selected Models"):
    for name in selected_models:
        model = models[name]
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        acc = accuracy_score(y_test, preds)
        results[name] = acc

    st.subheader("Model Accuracy Comparison")
    result_df = pd.DataFrame(list(results.items()), columns=["Model", "Accuracy"]).sort_values(by="Accuracy", ascending=False)
    st.dataframe(result_df)

    fig = px.bar(result_df, x="Model", y="Accuracy", color="Accuracy", title="Model Accuracy Comparison")
    st.plotly_chart(fig, use_container_width=True)

    best_model_name = max(results, key=results.get)
    st.success(f"Best performing model: {best_model_name} with accuracy {results[best_model_name]:.2f}")

if st.checkbox("Show Confusion Matrix for Best Model"):
    best_model = models[best_model_name]
    preds = best_model.predict(X_test)
    cm = confusion_matrix(y_test, preds)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    st.pyplot(fig)

st.subheader("Neural Network Model")

if st.button("Train Neural Network"):
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    nn_model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    history = nn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)

    test_loss, test_acc = nn_model.evaluate(X_test_scaled, y_test, verbose=0)
    st.success(f"Neural Network Test Accuracy: {test_acc:.2f}")

    fig, ax = plt.subplots(1, 2, figsize=(10, 4))
    ax[0].plot(history.history['accuracy'], label='Train Accuracy')
    ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    ax[0].legend()
    ax[0].set_title("Accuracy Over Epochs")

    ax[1].plot(history.history['loss'], label='Train Loss')
    ax[1].plot(history.history['val_loss'], label='Validation Loss')
    ax[1].legend()
    ax[1].set_title("Loss Over Epochs")

    st.pyplot(fig)
st.header("Natural Language Processing (NLP) and Sentiment Analysis")

text_input = st.text_area("Enter a sentence or short paragraph to analyze its sentiment:")
if st.button("Analyze Sentiment"):
    if text_input.strip() != "":
        sentiment = TextBlob(text_input).sentiment.polarity
        if sentiment > 0:
            st.success(f"Positive Sentiment ({sentiment:.2f})")
        elif sentiment < 0:
            st.error(f"Negative Sentiment ({sentiment:.2f})")
        else:
            st.warning("Neutral Sentiment (0.00)")
    else:
        st.warning("Please enter some text.")

st.header("Clustering and Data Patterns (K-Means + PCA)")

num_clusters = st.slider("Select number of clusters", 2, 10, 3)
scaled = StandardScaler().fit_transform(X)
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
labels = kmeans.fit_predict(scaled)
df['Cluster'] = labels

pca = PCA(2)
components = pca.fit_transform(scaled)
pca_df = pd.DataFrame(data=components, columns=["PC1", "PC2"])
pca_df['Cluster'] = labels

fig = px.scatter(pca_df, x="PC1", y="PC2", color=pca_df["Cluster"].astype(str),
                 title="K-Means Clustering with PCA Visualization")
st.plotly_chart(fig, use_container_width=True)

st.header("Make Predictions")

user_input = {}
for col in X.columns:
    val = st.number_input(f"Enter value for {col}", value=float(df[col].mean()))
    user_input[col] = val

user_df = pd.DataFrame([user_input])

if st.button("Predict Using Best Model"):
    best_model = models[max(results, key=results.get)]
    prediction = best_model.predict(user_df)[0]
    st.success(f"Predicted Class: {prediction}")

st.header("Summary Insights")

col1, col2 = st.columns(2)
with col1:
    st.subheader("Top Correlations")
    corr = df.corr()[target].sort_values(ascending=False)
    st.write(corr.head(10))

with col2:
    st.subheader("Feature Importance (Random Forest)")
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X_train, y_train)
    importances = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf.feature_importances_
    }).sort_values(by='Importance', ascending=False)
    fig = px.bar(importances.head(10), x='Feature', y='Importance', title='Top 10 Important Features')
    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")
st.markdown("**Developed for AI Mental Health & Lifestyle Research Dashboard**")

